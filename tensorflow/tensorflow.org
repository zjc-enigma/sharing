#+Title: Tensorflow introduction
#+Author: jiancheng.zhai
#+Email: jiancheng.pro@gmail.com
# #+OPTIONS: toc:nil reveal_mathjax:t
#+OPTIONS: ^:nil
#+OPTIONS: toc:nil num:nil
#+STARTUP: indent
#+REVEAL_THEME: white
#+REVEAL_TRANS: linear    
# #+REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/3.0.0/
#+REVEAL_ROOT: ~/Git/reveal.js
# #+SETUPFILE: ~/Git//org-html-themes/setup/theme-readtheorg.setup

* Introduction
* Compare with ...
* Install
** pip
*** conf
#+BEGIN_SRC conf
# ~/.pip/pip.conf
[global]
index-url = http://pypi.douban.com/simple 
trusted-host = pypi.douban.com
disable-pip-version-check = true
timeout = 120
#+END_SRC
*** install 
#+BEGIN_SRC sh
# python 3.5 with GPU support
export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow-0.11.0-py3-none-any.whl 
#+END_SRC
*** reference
https://www.tensorflow.org/versions/r0.11/get_started/os_setup#pip-installation
http://blog.csdn.net/gs_008/article/details/52833741 

** docker
* import tensorflow as tf
* Framework concept
** computational graph
*** build graph
*** run graph
** tensor
#+BEGIN_SRC python
3 # a rank 0 tensor; this is a scalar with shape []
[1. ,2., 3.] # a rank 1 tensor; this is a vector with shape [3]
[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]
[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]
#+END_SRC
** session

* Data structure
** tf.constant

#+BEGIN_SRC python
# tf.float16
# tf.float32
# tf.float64
node1 = tf.constant(3.0, tf.float32)
node2 = tf.constant(4.0) # also tf.float32 implicitly
print(node1, node2)
#+END_SRC

** tf.placeholder
#+BEGIN_SRC python
a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
adder_node = a + b  # + provides a shortcut for tf.add(a, b)
#+END_SRC

** tf.Variable
#+BEGIN_SRC python
W = tf.Variable([.3], tf.float32)
b = tf.Variable([-.3], tf.float32)
x = tf.placeholder(tf.float32)
linear_model = W * x + b
#+END_SRC

** Matrix

* Basic operation
** load / export Data
** tf.Session
#+BEGIN_SRC python
# sess = tf.InteractiveSession()
sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)
#+END_SRC

** tf.assign
#+BEGIN_SRC python
fixW = tf.assign(W, [-1.])
fixb = tf.assign(b, [1.])
#+END_SRC
** tf.square

** tf.reduce_sum
#+BEGIN_SRC python
loss = tf.reduce_sum(squared_deltas)
print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))
#+END_SRC

** tf.reduce_mean
#+BEGIN_SRC python
cross_entropy = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
#+END_SRC

** tf.add
#+BEGIN_SRC python
node3 = tf.add(node1, node2)
print("node3: ", node3)
print("sess.run(node3): ",sess.run(node3))
#+END_SRC

** tf.matmul
#+BEGIN_SRC python
#+END_SRC

** tf.equal

** tf.cast 
#+BEGIN_SRC python
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
#+END_SRC

** tf.gradients
*** 梯度
** 微分
** 积分

** hessian matrix
* Function
** tf.nn.softmax_cross_entropy_with_logits
#+BEGIN_SRC python
tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)
#+END_SRC

** tf.sigmoid

** tf.nn.softmax

* Optimization methods
** tf.train.GradientDescentOptimizer 
#+BEGIN_SRC python
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
#+END_SRC

* Demos
** Linear Regression - data
#+BEGIN_SRC python :session :exports both :results file
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
rng = np.random

# Parameters
learning_rate = 0.01
training_epochs = 1000
display_step = 50


# make training data
def create_toy_data(func, low=0, high=1., n=100, std=1.):
    # sample points
    x = np.random.uniform(low, high, n)
    t = func(x) + np.random.normal(scale=std, size=n)
    #t = func(x)
    return x, t


def func(x):
    return np.sin(2 * np.pi * x)


train_X, train_Y = create_toy_data(func)
n_samples = train_X.shape[0]
test_X = np.linspace(0, 1, 100)


plt.scatter(train_X,
            train_Y,
            alpha=0.5,
            color="blue",
            label="observation")

plt.plot(test_X,
        func(test_X),
        color="red",
        label="sin$(2\pi x)$")

plt.show()
img_save_path = "img/data_dist.png"
#+END_SRC 

** Linear Regression - model
#+BEGIN_SRC python :session

# tf Graph Input
X = tf.placeholder("float")
Y = tf.placeholder("float")

# Set model weights : init to random values
W = tf.Variable(rng.randn(), name="weight")
b = tf.Variable(rng.randn(), name="bias")
#+END_SRC 

\[ p(D) = \int_{-\infty}^{\infty} p(D|\mu)p(\mu)d\mu \neq \prod_{n=1}^{N} p(x_{n}) \]
#+BEGIN_SRC python :session
# Construct a linear model
pred = tf.add(tf.mul(X, W), b)
#+END_SRC 

# Mean squared error
cost = tf.reduce_sum(tf.pow(pred - Y, 2))/(2*n_samples)



* Tools
** tf.learn
** tf.contrib.learn
** tf.nn
** tensorflow board
** tensorflow cluster
** tensorflow serving

* Distribution
* Tips
** CUDA 加速
** numpy

* Q & A
* Concept
* Target function
* Loss function
* Cost function
* Error function
** cross entropy
** Mean squared error


* Accuracy
* 无约束优化问题
* 梯度下降
** 沿梯度方向，与等值面相切，不是直接指向最优解方向
** Batch gradient descent
** Mini-batch gradient descent
** Stochastic gradient descent
* 牛顿法
** 指向最优解方向
** 能找出驻点，而不仅仅是极大极小点
** 需要计算 Hessian 矩阵和其逆，计算量大
* 共轭梯度
** 不用计算 Hessian 逆
* 以上三种方法 batch 迭代？
* Adagrad
* Adadelta
* 无约束优化
* 含约束优化问题 
* 拉格朗日  - 等式约束
* KKT（Karush Kuhn Tucker） - 不等约束
** 需要满足 KKT 条件
** 拉格朗日的推广
